---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Settings

```{python vscode={'languageId': 'python'}}
# Desired percentage of NIL mentions
perc_nil = 0.1

# Number of batch to create
n_batch = 10

# Desired NIL mentions per batch (in dev and test)
desired_nil = 50

# Desired size of a single dev and test batch
dev_test_desired_batch_size = 10000

# Random state
random_state = 1234

# Output dir
outdir = 'incremental_dataset'
```

# Imports

```{python vscode={'languageId': 'python'}}
import pandas as pd
from tqdm import tqdm,trange
from glob import glob
import numpy as np
from sklearn.model_selection import StratifiedKFold
import os
import json
```

# Download


Download the original dataset from https://github.com/yasumasaonoe/ET4EL


# Extract it


```
tar -xzf unseen_mentions.tar.gz
```

You should be able to see a folder named `unseen_mentions` with the following content:
```
unseen_mentions/
unseen_mentions/dev.json
unseen_mentions/test.json
unseen_mentions/train
unseen_mentions/train/train_2.json
unseen_mentions/train/train_1.json
unseen_mentions/train/train_4.json
unseen_mentions/train/train_3.json
unseen_mentions/train/train_5.json
unseen_mentions/train/train_0.json
```


# Load Original Dataset

```{python vscode={'languageId': 'python'}}
train_files = glob('unseen_mentions/train/*.json')
train_files.sort()
train_files
```

```{python vscode={'languageId': 'python'}}
train_df = pd.DataFrame()
for file in tqdm(train_files):
    _temp = pd.read_json(file, lines=True)
    print(file, 'shape: ', _temp.shape)
    train_df = pd.concat([train_df, _temp], ignore_index=True)
```

```{python vscode={'languageId': 'python'}}
dev_df = pd.read_json('unseen_mentions/dev.json', lines=True)
```

```{python vscode={'languageId': 'python'}}
test_df = pd.read_json('unseen_mentions/test.json', lines=True)
```

# Proceed


### Calculate the absolute frequency with which each entity is mentioned

```{python vscode={'languageId': 'python'}}
mention_frequency = train_df.groupby('wikiId').size()
```

```{python vscode={'languageId': 'python'}}
mention_frequency
```

```{python vscode={'languageId': 'python'}}
freq_df = pd.DataFrame(mention_frequency)
freq_df.columns = ['freq']
freq_df.shape
```

```{python vscode={'languageId': 'python'}}
freq_df.head()
```

```{python vscode={'languageId': 'python'}}
# calculate the median frequency with which all the entities are mentioned
med_freq = np.median(freq_df['freq'])
med_freq
```

Calculate the probability that each entity is NIL

```{python vscode={'languageId': 'python'}}
np.random.seed(random_state)

freq_df['p_formula'] = perc_nil ** (freq_df['freq'] / med_freq)
s = np.random.uniform(0, 1, freq_df.shape[0])
freq_df['p_uniform'] = s
freq_df.head()
```

```{python vscode={'languageId': 'python'}}
freq_df['NIL'] = freq_df['p_uniform'] < freq_df['p_formula']
freq_df.head()
```

```{python vscode={'languageId': 'python'}}
print('N# NIL:', freq_df.eval('NIL').sum(), 'percentage:', freq_df.eval('NIL').sum() / freq_df.shape[0] * 100, '%')
```

```{python vscode={'languageId': 'python'}}
# Setting NIL freq to 0 so that when we split in batches each batch has the same number of NILs
freq_df.loc[freq_df['NIL'] == True, 'freq'] = 0
```

```{python vscode={'languageId': 'python'}}
train_df
```

```{python vscode={'languageId': 'python'}}
train_df_merged = train_df.join(freq_df, how='left', on='wikiId')
```

```{python vscode={'languageId': 'python'}}
print('Percentage of NIL mentions in train:', train_df_merged.eval('NIL').sum() / train_df_merged.shape[0] * 100)
```

# Propagate NIL mentions in test and dev

```{python vscode={'languageId': 'python'}}
dev_df_merged = dev_df.join(freq_df, how='left', on='wikiId')
```

```{python vscode={'languageId': 'python'}}
dev_df_merged['p_formula'] = dev_df_merged['p_formula'].fillna(-1)
dev_df_merged['p_uniform'] = dev_df_merged['p_uniform'].fillna(-1)
dev_df_merged['NIL'] = dev_df_merged['NIL'].fillna(False)
```

Compute entity frequencies in dev for a correct sampling

```{python vscode={'languageId': 'python'}}
mention_frequency_dev = dev_df_merged.query('~NIL').groupby('wikiId').size()
```

```{python vscode={'languageId': 'python'}}
mention_frequency_dev
```

```{python vscode={'languageId': 'python'}}
freq_df_dev = pd.DataFrame(mention_frequency_dev)
freq_df_dev.columns = ['freq']
freq_df_dev.shape
```

```{python vscode={'languageId': 'python'}}
freq_df_dev.head()
```

```{python vscode={'languageId': 'python'}}
dev_df_merged = dev_df_merged.drop(columns=['freq']).join(freq_df_dev, how='left', on='wikiId')
```

```{python vscode={'languageId': 'python'}}
# set NIL entities freq to 0 for stratifying the NIL class
dev_df_merged.loc[dev_df_merged['NIL'], 'freq'] = 0
```

```{python vscode={'languageId': 'python'}}
dev_df_merged['freq'] = dev_df_merged['freq'].astype(int)
```

```{python vscode={'languageId': 'python'}}
assert not dev_df_merged['NIL'].isna().any()
```

```{python vscode={'languageId': 'python'}}
print('dev NIL N#:', dev_df_merged.eval('NIL').sum(), 'Percentage:', dev_df_merged.eval('NIL').sum() / dev_df_merged.shape[0] * 100)
```

```{python vscode={'languageId': 'python'}}
test_df_merged = test_df.join(freq_df, how='left', on='wikiId')
```

```{python vscode={'languageId': 'python'}}
test_df_merged['p_formula'] = test_df_merged['p_formula'].fillna(-1)
test_df_merged['p_uniform'] = test_df_merged['p_uniform'].fillna(-1)
test_df_merged['NIL'] = test_df_merged['NIL'].fillna(False)
```

Compute entity frequencies in test for a correct sampling

```{python vscode={'languageId': 'python'}}
mention_frequency_test = test_df_merged.query('~NIL').groupby('wikiId').size()
```

```{python vscode={'languageId': 'python'}}
mention_frequency_test
```

```{python vscode={'languageId': 'python'}}
freq_df_test = pd.DataFrame(mention_frequency_test)
freq_df_test.columns = ['freq']
freq_df_test.shape
```

```{python vscode={'languageId': 'python'}}
freq_df_test.head()
```

```{python vscode={'languageId': 'python'}}
test_df_merged = test_df_merged.drop(columns=['freq']).join(freq_df_test, how='left', on='wikiId')
```

```{python vscode={'languageId': 'python'}}
# set NIL entities freq to 0 for stratifying the NIL class
test_df_merged.loc[test_df_merged['NIL'], 'freq'] = 0
```

```{python vscode={'languageId': 'python'}}
test_df_merged['freq'] = test_df_merged['freq'].astype(int)
```

```{python vscode={'languageId': 'python'}}
assert not test_df_merged['NIL'].isna().any()
```

```{python vscode={'languageId': 'python'}}
print('test NIL N#:', test_df_merged.eval('NIL').sum(), 'Percentage:', test_df_merged.eval('NIL').sum() / test_df_merged.shape[0] * 100)
```

# Transplant

```{python vscode={'languageId': 'python'}}
# I want 500 NIL in dev and test
# I want 100k total in both dev and test
```

```{python vscode={'languageId': 'python'}}
train_unique = train_df[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
dev_unique = dev_df[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
test_unique = test_df[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
# ensure no duplicates
assert not train_unique.duplicated().any()
assert not dev_unique.duplicated().any()
assert not test_unique.duplicated().any()
```

```{python vscode={'languageId': 'python'}}
# ensure unseen mentions holds
assert not pd.concat([train_unique, dev_unique]).duplicated().any()
assert not pd.concat([train_unique, test_unique]).duplicated().any()
assert not pd.concat([dev_unique, test_unique]).duplicated().any()
```

## NIL

```{python vscode={'languageId': 'python'}}
# prendo n menzioni NIL dal train che non siano nel dev/test e ce le metto
```

```{python vscode={'languageId': 'python'}}
train_df_nil = train_df_merged.query('NIL')
```

```{python vscode={'languageId': 'python'}}
desired_nil_total = desired_nil * n_batch
nil_to_add_dev = desired_nil_total - dev_df_merged.eval('NIL').sum()
nil_to_add_dev
```

```{python vscode={'languageId': 'python'}}
nil_to_add_test = desired_nil_total - test_df_merged.eval('NIL').sum()
nil_to_add_test
```

```{python vscode={'languageId': 'python'}}
def get_subset_sum_ge(df, sum_, random_state):
    """
    Gets a subset whose sum is greater than sum_
    """
    
    #print(sum_)
    
    if df.shape[0] == 0:
        return None
    
    # get a sample
    item = df.sample(n=1, random_state=random_state)
    random_state += 1
    df = df.drop(item.index) # remove just extracted item
    
    subset = item.index

    current_sum = item['count'].values[0]
    while current_sum < sum_:
        print(f'\r{current_sum}/{sum_}',end='')
        # get a sample
        item = df.sample(n=1, random_state=random_state)
        random_state += 1
        df = df.drop(item.index) # remove just extracted item

        current_sum += item['count'].values[0]
        
        subset = subset.union(item.index)
    print()
    return subset
```

```{python vscode={'languageId': 'python'}}
train_df_nil_unique_count = pd.DataFrame(
    train_df_nil[['wikiId', 'word']].value_counts(), columns=['count']).reset_index()
train_df_nil_unique_count.head()
```

```{python vscode={'languageId': 'python'}}
sample_transplant_to_dev = get_subset_sum_ge(train_df_nil_unique_count, nil_to_add_dev, random_state)
print(train_df_nil_unique_count.loc[sample_transplant_to_dev]['count'].sum(), '>=', nil_to_add_dev)
assert train_df_nil_unique_count.loc[sample_transplant_to_dev]['count'].sum() >= nil_to_add_dev
```

```{python vscode={'languageId': 'python'}}
# remove sample_transplant to dev before extracting a sample for test
train_df_nil_unique_count_no_dev = train_df_nil_unique_count.drop(sample_transplant_to_dev)
sample_transplant_to_test = get_subset_sum_ge(train_df_nil_unique_count_no_dev, nil_to_add_test, random_state+10)
print(train_df_nil_unique_count_no_dev.loc[sample_transplant_to_test]['count'].sum(), '>=', nil_to_add_test)
assert train_df_nil_unique_count_no_dev.loc[sample_transplant_to_test]['count'].sum() >= nil_to_add_test
```

## Transplant NIL to dev and test


### Dev

```{python vscode={'languageId': 'python'}}
mentions_to_dev = train_df_nil.join(
    train_df_nil_unique_count.loc[sample_transplant_to_dev].set_index(['wikiId', 'word']),
    on=['wikiId', 'word'], how='inner')
mentions_to_dev = mentions_to_dev.drop(columns=['count'])
```

```{python vscode={'languageId': 'python'}}
dev_df_transplant = pd.concat([dev_df_merged, mentions_to_dev], ignore_index=True)
dev_df_transplant = dev_df_transplant.sample(frac=1, random_state=random_state) # randomize
```

```{python vscode={'languageId': 'python'}}
dev_df_transplant.shape
```

```{python vscode={'languageId': 'python'}}
print(dev_df_transplant.eval('NIL').sum(), '>=', desired_nil_total)
assert dev_df_transplant.eval('NIL').sum() >= desired_nil_total
```

### Test

```{python vscode={'languageId': 'python'}}
mentions_to_test = train_df_nil.join(
    train_df_nil_unique_count.loc[sample_transplant_to_test].set_index(['wikiId', 'word']),
    on=['wikiId', 'word'], how='inner')
mentions_to_test = mentions_to_test.drop(columns=['count'])
```

```{python vscode={'languageId': 'python'}}
test_df_transplant = pd.concat([test_df_merged, mentions_to_test], ignore_index=True)
test_df_transplant = test_df_transplant.sample(frac=1, random_state=random_state) # randomize
```

```{python vscode={'languageId': 'python'}}
test_df_transplant.shape
```

```{python vscode={'languageId': 'python'}}
print(test_df_transplant.eval('NIL').sum(), '>=', desired_nil_total)
assert test_df_transplant.eval('NIL').sum() >= desired_nil_total
```

### Remove from train

```{python vscode={'languageId': 'python'}}
train_df_transplant = train_df_merged.drop(mentions_to_dev.index.union(mentions_to_test.index))
```

```{python vscode={'languageId': 'python'}}
train_unique_transplant = train_df_transplant[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
dev_unique_transplant = dev_df_transplant[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
test_unique_transplant = test_df_transplant[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
# ensure no duplicates
assert not train_unique_transplant.duplicated().any()
assert not dev_unique_transplant.duplicated().any()
assert not test_unique_transplant.duplicated().any()
```

```{python vscode={'languageId': 'python'}}
# ensure unseen mentions holds
assert not pd.concat([train_unique_transplant, dev_unique_transplant]).duplicated().any()
assert not pd.concat([train_unique_transplant, test_unique_transplant]).duplicated().any()
assert not pd.concat([dev_unique_transplant, test_unique_transplant]).duplicated().any()
```

```{python vscode={'languageId': 'python'}}
print('Train', train_df_transplant.shape)
print('Dev', dev_df_transplant.shape)
print('Test', test_df_transplant.shape)
```

# Transplant not-NIL mentions

```{python vscode={'languageId': 'python'}}
# prendo n menzioni NIL dal train che non siano nel dev/test e ce le metto
```

```{python vscode={'languageId': 'python'}}
train_df_not_nil = train_df_merged.query('~NIL')
```

```{python vscode={'languageId': 'python'}}
desired_total = dev_test_desired_batch_size * n_batch
```

```{python vscode={'languageId': 'python'}}
not_nil_to_add_dev = desired_total - dev_df_transplant.shape[0]
not_nil_to_add_dev
```

```{python vscode={'languageId': 'python'}}
not_nil_to_add_test = desired_total - test_df_transplant.shape[0]
not_nil_to_add_test
```

```{python vscode={'languageId': 'python'}}
train_df_not_nil_unique_count = pd.DataFrame(
    train_df_not_nil[['wikiId', 'word']].value_counts(), columns=['count']).reset_index()
train_df_not_nil_unique_count.head()
```

```{python vscode={'languageId': 'python'}}
sample_transplant_to_dev_not_nil = get_subset_sum_ge(train_df_not_nil_unique_count, not_nil_to_add_dev, random_state)
```

```{python vscode={'languageId': 'python'}}
print(train_df_not_nil_unique_count.loc[sample_transplant_to_dev_not_nil]['count'].sum(), ">=", not_nil_to_add_dev)
assert train_df_not_nil_unique_count.loc[sample_transplant_to_dev_not_nil]['count'].sum() >= not_nil_to_add_dev
```

```{python vscode={'languageId': 'python'}}
# remove sample_transplant to dev before extracting a sample for test
train_df_not_nil_unique_count_no_dev = train_df_not_nil_unique_count.drop(sample_transplant_to_dev_not_nil)
sample_transplant_to_test_not_nil = get_subset_sum_ge(train_df_not_nil_unique_count_no_dev, not_nil_to_add_test, random_state+10)
```

```{python vscode={'languageId': 'python'}}
print(train_df_not_nil_unique_count_no_dev.loc[sample_transplant_to_test_not_nil]['count'].sum(), ">=", not_nil_to_add_test)
assert train_df_not_nil_unique_count_no_dev.loc[sample_transplant_to_test_not_nil]['count'].sum() >= not_nil_to_add_test
```

### Dev

```{python vscode={'languageId': 'python'}}
mentions_to_dev_not_nil = train_df_not_nil.join(
    train_df_not_nil_unique_count.loc[sample_transplant_to_dev_not_nil].set_index(['wikiId', 'word']),
    on=['wikiId', 'word'], how='inner')
mentions_to_dev_not_nil = mentions_to_dev_not_nil.drop(columns=['count'])
```

```{python vscode={'languageId': 'python'}}
assert not mentions_to_dev_not_nil.eval('NIL').any()
```

```{python vscode={'languageId': 'python'}}
dev_df_transplant_final = pd.concat([dev_df_transplant, mentions_to_dev_not_nil], ignore_index=True)
dev_df_transplant_final = dev_df_transplant_final.sample(frac=1, random_state=random_state) # randomize
```

```{python vscode={'languageId': 'python'}}
dev_df_transplant_final.shape
```

```{python vscode={'languageId': 'python'}}
assert dev_df_transplant_final.eval('NIL').sum() >= desired_nil_total
assert dev_df_transplant_final.shape[0] >= desired_total
```

### Test

```{python vscode={'languageId': 'python'}}
mentions_to_test_not_nil = train_df_not_nil.join(
    train_df_not_nil_unique_count.loc[sample_transplant_to_test_not_nil].set_index(['wikiId', 'word']),
    on=['wikiId', 'word'], how='inner')
mentions_to_test_not_nil = mentions_to_test_not_nil.drop(columns=['count'])
```

```{python vscode={'languageId': 'python'}}
test_df_transplant_final = pd.concat([test_df_transplant, mentions_to_test_not_nil], ignore_index=True)
test_df_transplant_final = test_df_transplant_final.sample(frac=1, random_state=random_state) # randomize
```

```{python vscode={'languageId': 'python'}}
test_df_transplant_final.shape
```

```{python vscode={'languageId': 'python'}}
assert test_df_transplant_final.eval('NIL').sum() >= desired_nil_total
assert test_df_transplant_final.shape[0] >= desired_total
```

### Remove from train

```{python vscode={'languageId': 'python'}}
train_df_transplant_final = train_df_transplant.drop(mentions_to_dev_not_nil.index.union(mentions_to_test_not_nil.index))
```

```{python vscode={'languageId': 'python'}}
train_unique_transplant_final = train_df_transplant_final[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
dev_unique_transplant_final = dev_df_transplant_final[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
test_unique_transplant_final = test_df_transplant_final[['word', 'wikiId']].drop_duplicates(keep='first')
```

```{python vscode={'languageId': 'python'}}
# ensure no duplicates
assert not train_unique_transplant_final.duplicated().any()
assert not dev_unique_transplant_final.duplicated().any()
assert not test_unique_transplant_final.duplicated().any()
```

```{python vscode={'languageId': 'python'}}
# ensure unseen mentions holds
assert not pd.concat([train_unique_transplant_final, dev_unique_transplant_final]).duplicated().any()
assert not pd.concat([train_unique_transplant_final, test_unique_transplant_final]).duplicated().any()
assert not pd.concat([dev_unique_transplant_final, test_unique_transplant_final]).duplicated().any()
```

```{python vscode={'languageId': 'python'}}
print('Train', train_df_transplant_final.shape)
print('Dev', dev_df_transplant_final.shape)
print('Test', test_df_transplant_final.shape)
```

# Remove samples from dev test to reach the desired number
If required, here it is possible to remove samples from the dev and test sets to reach a precise number of samples.


# Divide train in batches

```{python vscode={'languageId': 'python'}}
skf = StratifiedKFold(n_splits=n_batch, shuffle=True, random_state=random_state)
```

## Train

```{python vscode={'languageId': 'python'}}
train_df_transplant_final = train_df_transplant_final.reset_index(drop=True)
```

```{python vscode={'languageId': 'python'}}
train_batch_indexes = []
# the NIL class has freq = 0 so it is fairly distributed among the batches
for _, _index in skf.split(np.zeros(train_df_transplant_final.shape[0]), train_df_transplant_final['freq']):
    train_batch_indexes.append(_index)
```

```{python vscode={'languageId': 'python'}}
for i, batch in enumerate(train_batch_indexes):
    train_df_transplant_final.loc[batch, 'batch'] = i
train_df_transplant_final['batch'] = train_df_transplant_final['batch'].astype(int)
```

```{python vscode={'languageId': 'python'}}
train_df_transplant_final.head()
```

## Dev

```{python vscode={'languageId': 'python'}}
dev_df_transplant_final = dev_df_transplant_final.reset_index(drop=True)
```

```{python vscode={'languageId': 'python'}}
dev_batch_indexes = []
# the NIL class has freq = 0 so it is fairly distributed among the batches
for _, _index in skf.split(np.zeros(dev_df_transplant_final.shape[0]), dev_df_transplant_final['freq']):
    dev_batch_indexes.append(_index)
```

```{python vscode={'languageId': 'python'}}
for i, batch in enumerate(dev_batch_indexes):
    dev_df_transplant_final.loc[batch, 'batch'] = i
dev_df_transplant_final['batch'] = dev_df_transplant_final['batch'].astype(int)
```

```{python vscode={'languageId': 'python'}}
dev_df_transplant_final.head()
```

## Test

```{python vscode={'languageId': 'python'}}
test_df_transplant_final = test_df_transplant_final.reset_index(drop=True)
```

```{python vscode={'languageId': 'python'}}
test_batch_indexes = []
# the NIL class has freq = 0 so it is fairly distributed among the batches
for _, _index in skf.split(np.zeros(test_df_transplant_final.shape[0]), test_df_transplant_final['freq']):
    test_batch_indexes.append(_index)
```

```{python vscode={'languageId': 'python'}}
for i, batch in enumerate(test_batch_indexes):
    test_df_transplant_final.loc[batch, 'batch'] = i
test_df_transplant_final['batch'] = test_df_transplant_final['batch'].astype(int)
```

```{python vscode={'languageId': 'python'}}
test_df_transplant_final.head()
```

# Prepare for BLINK

```{python vscode={'languageId': 'python'}}
train_df_transplant_final = train_df_transplant_final.rename(columns = {
    'left_context_text': 'context_left',
    'word': 'mention',
    'right_context_text': 'context_right',
    'ex_id': 'query_id',
    'url':'label_id',
    'wikiId':'Wikipedia_ID',
    'wikiurl':'Wikipedia_URL',
    'y_title': 'Wikipedia_title'
})
```

```{python vscode={'languageId': 'python'}}
dev_df_transplant_final = dev_df_transplant_final.rename(columns = {
    'left_context_text': 'context_left',
    'word': 'mention',
    'right_context_text': 'context_right',
    'ex_id': 'query_id',
    'url':'label_id',
    'wikiId':'Wikipedia_ID',
    'wikiurl':'Wikipedia_URL',
    'y_title': 'Wikipedia_title'
})
```

```{python vscode={'languageId': 'python'}}
test_df_transplant_final = test_df_transplant_final.rename(columns = {
    'left_context_text': 'context_left',
    'word': 'mention',
    'right_context_text': 'context_right',
    'ex_id': 'query_id',
    'url':'label_id',
    'wikiId':'Wikipedia_ID',
    'wikiurl':'Wikipedia_URL',
    'y_title': 'Wikipedia_title'
})
```

# Save to disk

```{python vscode={'languageId': 'python'}}
train_basedir = os.path.join(outdir, 'train')
os.makedirs(train_basedir, exist_ok=True)
print('Saving train...')
for batch in range(n_batch):
    print('Batch {} of {}'.format(batch + 1, n_batch))
    train_batch = train_df_transplant_final[train_df_transplant_final['batch'] == batch]
    with open(os.path.join(train_basedir, 'train_{}.jsonl'.format(batch)), 'w') as fd:
        for i, row in tqdm(train_batch.iterrows(), total=train_batch.shape[0]):
            fd.write(row.to_json())
            fd.write('\n')
```

```{python vscode={'languageId': 'python'}}
dev_basedir = os.path.join(outdir, 'dev')
os.makedirs(dev_basedir, exist_ok=True)
print('Saving dev...')
for batch in range(n_batch):
    print('Batch {} of {}'.format(batch + 1, n_batch))
    dev_batch = dev_df_transplant_final[dev_df_transplant_final['batch'] == batch]
    with open(os.path.join(dev_basedir, 'dev_{}.jsonl'.format(batch)), 'w') as fd:
        for i, row in tqdm(dev_batch.iterrows(), total=dev_batch.shape[0]):
            fd.write(row.to_json())
            fd.write('\n')
```

```{python vscode={'languageId': 'python'}}
test_basedir = os.path.join(outdir, 'test')
os.makedirs(test_basedir, exist_ok=True)
print('Saving test...')
for batch in range(n_batch):
    print('Batch {} of {}'.format(batch + 1, n_batch))
    test_batch = test_df_transplant_final[test_df_transplant_final['batch'] == batch]
    with open(os.path.join(test_basedir, 'test_{}.jsonl'.format(batch)), 'w') as fd:
        for i, row in tqdm(test_batch.iterrows(), total=test_batch.shape[0]):
            fd.write(row.to_json())
            fd.write('\n')
```

# Statistics

```{python vscode={'languageId': 'python'}}
report = []
```

```{python vscode={'languageId': 'python'}}
for batch in trange(n_batch):
    train_batch = train_df_transplant_final[train_df_transplant_final['batch'] == batch]
    dev_batch = dev_df_transplant_final[dev_df_transplant_final['batch'] == batch]
    test_batch = test_df_transplant_final[test_df_transplant_final['batch'] == batch]
    
    batch_report = {}
    batch_report['batch'] = batch
    
    # mentions
    batch_report['train_mentions'] = train_batch.shape[0]
    batch_report['dev_mentions'] = dev_batch.shape[0]
    batch_report['test_mentions'] = test_batch.shape[0]

    # entities
    batch_report['train_entities'] = train_batch['Wikipedia_ID'].drop_duplicates().shape[0]
    batch_report['dev_entities'] = dev_batch['Wikipedia_ID'].drop_duplicates().shape[0]
    batch_report['test_entities'] = test_batch['Wikipedia_ID'].drop_duplicates().shape[0]

    # NIL mentions
    batch_report['train_nil_mentions'] = train_batch.eval('NIL').sum()
    batch_report['dev_nil_mentions'] = dev_batch.eval('NIL').sum()
    batch_report['test_nil_mentions'] = test_batch.eval('NIL').sum()
    
    # not-NIL mentions
    batch_report['train_not_nil_mentions'] = batch_report['train_mentions'] - batch_report['train_nil_mentions']
    batch_report['dev_not_nil_mentions'] = batch_report['dev_mentions'] - batch_report['dev_nil_mentions']
    batch_report['test_not_nil_mentions'] = batch_report['test_mentions'] - batch_report['test_nil_mentions']

    # NIL entities
    batch_report['train_nil_entities'] = train_batch.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]
    batch_report['dev_nil_entities'] = dev_batch.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]
    batch_report['test_nil_entities'] = test_batch.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]
    
    # not-NIL entities
    batch_report['train_not_nil_entities'] = batch_report['train_entities'] - batch_report['train_nil_entities']
    batch_report['dev_not_nil_entities'] = batch_report['dev_entities'] - batch_report['dev_nil_entities']
    batch_report['test_not_nil_entities'] = batch_report['test_entities'] - batch_report['test_nil_entities']
    
    # NIL entities present in a previous batch
    train_indexes = set(train_batch.query('NIL')['Wikipedia_ID'].drop_duplicates()
           ).intersection(
            # the set of all the nil entities found in the previous batches
            set(train_df_transplant_final.query('NIL and batch < {}'.format(batch))['Wikipedia_ID'].drop_duplicates())
        )
    batch_report['train_nil_entities_found_in_previous_batch'] = len(train_indexes)
    
    dev_indexes = set(dev_batch.query('NIL')['Wikipedia_ID'].drop_duplicates()
           ).intersection(
            # the set of all the nil entities found in the previous batches
            set(dev_df_transplant_final.query('NIL and batch < {}'.format(batch))['Wikipedia_ID'].drop_duplicates())
        )
    batch_report['dev_nil_entities_found_in_previous_batch'] = len(dev_indexes)
    
    test_indexes = set(test_batch.query('NIL')['Wikipedia_ID'].drop_duplicates()
           ).intersection(
            # the set of all the nil entities found in the previous batches
            set(test_df_transplant_final.query('NIL and batch < {}'.format(batch))['Wikipedia_ID'].drop_duplicates())
        )
    batch_report['test_nil_entities_found_in_previous_batch'] = len(test_indexes)
    
    # NIL mentions present in a previous batch
    batch_report['train_nil_mentions_found_in_previous_batch'] = train_batch['Wikipedia_ID'].isin(train_indexes).sum()
    batch_report['dev_nil_mentions_found_in_previous_batch'] = dev_batch['Wikipedia_ID'].isin(dev_indexes).sum()
    batch_report['test_nil_mentions_found_in_previous_batch'] = test_batch['Wikipedia_ID'].isin(test_indexes).sum()
    
    report.append(batch_report)
```

```{python vscode={'languageId': 'python'}}
all_dataset = {}
all_dataset['batch'] = 'ALL'

# mentions
all_dataset['train_mentions'] = train_df_transplant_final.shape[0]
all_dataset['dev_mentions'] = dev_df_transplant_final.shape[0]
all_dataset['test_mentions'] = test_df_transplant_final.shape[0]

# entities
all_dataset['train_entities'] = train_df_transplant_final['Wikipedia_ID'].drop_duplicates().shape[0]
all_dataset['dev_entities'] = dev_df_transplant_final['Wikipedia_ID'].drop_duplicates().shape[0]
all_dataset['test_entities'] = test_df_transplant_final['Wikipedia_ID'].drop_duplicates().shape[0]

# NIL mentions
all_dataset['train_nil_mentions'] = train_df_transplant_final.eval('NIL').sum()
all_dataset['dev_nil_mentions'] = dev_df_transplant_final.eval('NIL').sum()
all_dataset['test_nil_mentions'] = test_df_transplant_final.eval('NIL').sum()

# not-NIL mentions
all_dataset['train_not_nil_mentions'] = all_dataset['train_mentions'] - all_dataset['train_nil_mentions']
all_dataset['dev_not_nil_mentions'] = all_dataset['dev_mentions'] - all_dataset['dev_nil_mentions']
all_dataset['test_not_nil_mentions'] = all_dataset['test_mentions'] - all_dataset['test_nil_mentions']

# NIL entities
all_dataset['train_nil_entities'] = train_df_transplant_final.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]
all_dataset['dev_nil_entities'] = dev_df_transplant_final.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]
all_dataset['test_nil_entities'] = test_df_transplant_final.query('NIL')['Wikipedia_ID'].drop_duplicates().shape[0]

# not-NIL entities
all_dataset['train_not_nil_entities'] = all_dataset['train_entities'] - all_dataset['train_nil_entities']
all_dataset['dev_not_nil_entities'] = all_dataset['dev_entities'] - all_dataset['dev_nil_entities']
all_dataset['test_not_nil_entities'] = all_dataset['test_entities'] - all_dataset['test_nil_entities']

# NIL entities present in a previous batch
all_dataset['train_nil_entities_found_in_previous_batch'] = 0
all_dataset['dev_nil_entities_found_in_previous_batch'] = 0
all_dataset['test_nil_entities_found_in_previous_batch'] = 0

# NIL mentions present in a previous batch
all_dataset['train_nil_mentions_found_in_previous_batch'] = 0
all_dataset['dev_nil_mentions_found_in_previous_batch'] = 0
all_dataset['test_nil_mentions_found_in_previous_batch'] = 0

report.append(all_dataset)
```

```{python vscode={'languageId': 'python'}}
original_dataset = {}
original_dataset['batch'] = 'ORIGINAL'

# mentions
original_dataset['train_mentions'] = train_df.shape[0]
original_dataset['dev_mentions'] = dev_df.shape[0]
original_dataset['test_mentions'] = test_df.shape[0]

# entities
original_dataset['train_entities'] = train_df['wikiId'].drop_duplicates().shape[0]
original_dataset['dev_entities'] = dev_df['wikiId'].drop_duplicates().shape[0]
original_dataset['test_entities'] = test_df['wikiId'].drop_duplicates().shape[0]

# NIL mentions
original_dataset['train_nil_mentions'] =  0
original_dataset['dev_nil_mentions'] = 0
original_dataset['test_nil_mentions'] = 0

# not-NIL mentions
original_dataset['train_not_nil_mentions'] = original_dataset['train_mentions'] - original_dataset['train_nil_mentions']
original_dataset['dev_not_nil_mentions'] = original_dataset['dev_mentions'] - original_dataset['dev_nil_mentions']
original_dataset['test_not_nil_mentions'] = original_dataset['test_mentions'] - original_dataset['test_nil_mentions']

# NIL entities
original_dataset['train_nil_entities'] = 0
original_dataset['dev_nil_entities'] = 0
original_dataset['test_nil_entities'] = 0

# not-NIL entities
original_dataset['train_not_nil_entities'] = original_dataset['train_entities'] - original_dataset['train_nil_entities']
original_dataset['dev_not_nil_entities'] = original_dataset['dev_entities'] - original_dataset['dev_nil_entities']
original_dataset['test_not_nil_entities'] = original_dataset['test_entities'] - original_dataset['test_nil_entities']

# NIL entities present in a previous batch
original_dataset['train_nil_entities_found_in_previous_batch'] = 0
original_dataset['dev_nil_entities_found_in_previous_batch'] = 0
original_dataset['test_nil_entities_found_in_previous_batch'] = 0

# NIL mentions present in a previous batch
original_dataset['train_nil_mentions_found_in_previous_batch'] = 0
original_dataset['dev_nil_mentions_found_in_previous_batch'] = 0
original_dataset['test_nil_mentions_found_in_previous_batch'] = 0

report.append(original_dataset)
```

```{python vscode={'languageId': 'python'}}
report_df = pd.DataFrame(report)
report_df
```

## Save statistics

```{python vscode={'languageId': 'python'}}
stats_basedir = os.path.join(outdir, 'statistics')
os.makedirs(stats_basedir, exist_ok=True)
```

```{python vscode={'languageId': 'python'}}
report_df.to_csv(os.path.join(stats_basedir, 'statistics.csv'))
```

 # Remove NIL entities from the KB

```{python vscode={'languageId': 'python'}}
NIL_entities = []
NIL_entities.extend(
    train_df_transplant_final.query('NIL')['Wikipedia_ID'].tolist())
NIL_entities.extend(
    dev_df_transplant_final.query('NIL')['Wikipedia_ID'].tolist())
NIL_entities.extend(
    test_df_transplant_final.query('NIL')['Wikipedia_ID'].tolist())
```

```{python vscode={'languageId': 'python'}}
NIL_entities = set(NIL_entities) # remove duplicates
```

```{python vscode={'languageId': 'python'}}
sql_list = ",".join(str(i) for i in NIL_entities)
indexer = 10 # ensure to put the correct indexer
sql_query = 'DELETE FROM entities WHERE indexer = {} AND wikipedia_id in ({});'.format(indexer, sql_list)
```

```{python vscode={'languageId': 'python'}}
with open(os.path.join(outdir, 'delete_nil_entities.sql'), 'w') as fd:
    fd.write(sql_query + '\n')
```

Now run the sql query in the database (e.g.):
```
sudo docker-compose exec -T postgres psql -U postgres < /path/to/incremental_dataset/delete_nil_entities.sql
```

At this point NIL entities are removed from the db and even if retrieved by the faiss indexer they are discarded.
